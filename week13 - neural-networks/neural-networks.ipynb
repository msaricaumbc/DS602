{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks and Deep Learning\n",
    "This will briefly touch on some of the high level concepts with Neural Networks. See the resources for more information on each topic.\n",
    "\n",
    "## Agenda\n",
    "- Intro to ANNs\n",
    "- MLP with scikit-learn.    \n",
    "- Keras.  \n",
    "\n",
    "If you have issues viewing the notebook here, use https://nbviewer.org.\n",
    "\n",
    "## General Resources\n",
    "[Raschka's videos on deep learning](https://sebastianraschka.com/blog/2021/dl-course.html)\n",
    "<br>[Large-scale multi-label text classification with Keras](https://keras.io/examples/nlp/multi_label_classification/)\n",
    "<br>[TensorFlow](https://www.tensorflow.org)\n",
    "<br>[IBM - What are neural networks?](https://www.ibm.com/cloud/learn/neural-networks)\n",
    "<br>[Stanford CS230 Neural Networks](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)\n",
    "<br>[Grokking Deep Learning](https://github.com/iamtrask/Grokking-Deep-Learning)\n",
    "<br>[Origins of Deep Learning](https://arxiv.org/pdf/1702.07800.pdf)\n",
    "<br>[Andrew Ng - What is a Neural Network?](https://www.youtube.com/watch?v=n1l-9lIMW7E&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=3)\n",
    "<br>[TensorFlow Clothing Classification](https://www.tensorflow.org/tutorials/keras/classification)\n",
    "<br>[TensorFlow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=7,7,7,7,7&seed=0.02892&showTestData=true&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=true&ySquared=true&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&problem_hide=false&batchSize_hide=false&percTrainData_hide=false&numHiddenLayers_hide=false&playButton_hide=false&learningRate_hide=false&regularizationRate_hide=true&regularization_hide=true)\n",
    "<br>[MIT Introduction to Deep Learning](http://introtodeeplearning.com/slides/6S191_MIT_DeepLearning_L1.pdf)\n",
    "<br>[TensorFlow Data Pipelines](https://www.tensorflow.org/guide/data)\n",
    "<br>[DeepLearning AI Videos](https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w)\n",
    "<br>[Grid Search in Keras](https://keras.io/guides/keras_tuner/getting_started/)\n",
    "<br>[Deep Learning Book - Feedforward Neural Networks](https://www.deeplearningbook.org/contents/mlp.html)\n",
    "<br>[Fashion MNIST](https://www.tensorflow.org/tutorials/keras/classification)\n",
    "<br>[Keras Tuner](https://keras.io/keras_tuner/)\n",
    "\n",
    "### Convolutional\n",
    "[Convolutional NN Example](https://www.tensorflow.org/tutorials/images/cnn)\n",
    "\n",
    "### Recurrent\n",
    "[IBM - What are Recurrent Neural Networks?](https://www.ibm.com/cloud/learn/recurrent-neural-networks)\n",
    "<br>[Recurrent Neural Networks with TensorFlow](https://www.tensorflow.org/guide/keras/rnn)\n",
    "\n",
    "### Embeddings and autoencoders\n",
    "[Keras Embedding](https://keras.io/api/layers/core_layers/embedding/)\n",
    "<br>[Machine Learning Mastery Post on Embedding](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)\n",
    "<br>[Autoencoders from Deep Learning](https://www.deeplearningbook.org/contents/autoencoders.html<br>[])\n",
    "\n",
    "### More Advanced Neural Networks\n",
    "[Knowledge Graphs with Ampligraph](https://docs.ampligraph.org/en/1.4.0/)\n",
    "\n",
    "\n",
    "# Neural Networks and Deep Learning\n",
    "- This will introduce basic concepts regarding neural networks and is by no-means comprehensive.  \n",
    "- Neural networks can cover multiple full courses, so this only scratches the surface.  \n",
    "- See the readings and resources section for resources for more information.\n",
    "\n",
    "## Artificial Neural Networks (ANN)\n",
    "- Introduced in 1943, based on the concept of biological neurons.  \n",
    "- Generally used for image, speech, and text classification.  \n",
    "- It can be used in similar cases as we've seen for other examples, though the performance is generally similar to other models we've talked about.\n",
    "  \n",
    "## Biological Neurons\n",
    "\n",
    "<img src='files/diagrams/brain.png' style='width: 600px'>\n",
    "\n",
    "[Image source Machine Learning with Python 3rd Edition Pg. 20](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch02/images)\n",
    "\n",
    "- Built upon how the brain works to intake information, process and extrapolate it, and make decisions.  \n",
    "- Neurons can make very complex decisions.  \n",
    "- Artificial neurons perform various logical computations:\n",
    "\n",
    "<img src='files/diagrams/neuron-logic.png' stype='width: 500px'>\n",
    "\n",
    "- The brain uses `neurons`, but neural networks approximate this bahavior with `units` and `layers`.  \n",
    "- Neural networks have been around since the 1940s, but have become more popular in the last 20 years, coinciding with advancements in computing.  \n",
    "- The revolution really started in the 1980s, after a technique called backpropagation was developed, which makes deep neural networks much more efficient to compute.  \n",
    "\n",
    "## The Potential\n",
    "- Realistic potential in changing how humans live their lives:  \n",
    "    - Self-driving cars. \n",
    "    - Universal language translaters.  \n",
    "    - Semi-autonomous robots for manual/menial work.  \n",
    "    - Vaccine development.  \n",
    "    - \"Metaverse\"  \n",
    "    - ...  \n",
    "- Not quite there yet though...   \n",
    "    - Intuition without hard data.  \n",
    "    - Computing.  \n",
    "    - Training data with labels.  \n",
    "    - Ethical decisions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer (aka `Logistic Regression`)\n",
    "- Simpliest ANN architecture.  \n",
    "- Input layer and output layer.  \n",
    "- The activation function normalizes the output. Logistic regression uses a sigmoid function, but there are others that may be more robust for your neural network models.  \n",
    "- Instead of coefficients we will call these weights.  \n",
    "- The constant ($B_0$) is referred to as a `bias unit`.  \n",
    "\n",
    "<img src='files/diagrams/singlelayer.png' style='width: 500px'>\n",
    "\n",
    "\n",
    "[Image source Machine Learning with Python 3rd Edition Pg. 386](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch12)\n",
    "\n",
    "- The logistic regression model we manually created in [Week 6](https://github.com/appliedecon/data602-lectures/tree/main/week06) is essentially a single-layer neural network that uses a sigmoid activation function.\n",
    "\n",
    "We can think of this `single layer` as $y=f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layers\n",
    "<img src='files/diagrams/multilayer.png' style='width: 600px'>\n",
    "\n",
    "\n",
    "[Image source Machine Learning with Python 3rd Edition Pg. 388](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch12)\n",
    "\n",
    "- Many different types of neural networks. This is a feedforward, fully connected network, also called a `Multi-layer Perceptron (MLP)`.  \n",
    "- Think of these as nested functions, with a 3-layer specified as:\n",
    "$$\n",
    "y = f_{NN}(x)=f_3(f_2(f_1(x)))\n",
    "$$\n",
    "\n",
    "- Allows complex, non-linear, relationships to be learned. However, the non-linearity is approximated by a series of nested linear functions.    \n",
    "- $a_0^{(in)}$ and $A_0^{(h)}$ are bias units, and will equal $1$.  \n",
    "- The output of each unit gets passed into an activation function to normalize the output and allow for non-linear approximations.  \n",
    "- If there was not an activation functions, chains of linear combinations will only result in a linear transformation and that wouldn't be able to capture complexities of higher-order problems, i.e., a deep network without activation functions would be the same as a single-layer network.  \n",
    "\n",
    "Recall:\n",
    "$$\n",
    "y = f_{NN}(x)=f_3(f_2(f_1(x)))\n",
    "$$\n",
    "\n",
    "Individually these functions will be represented as:\n",
    "$$\n",
    "f_l(z)=g_l(W_{lz}+b_t)\n",
    "$$\n",
    "\n",
    "- $l$ is the layer\n",
    "- $g_l$ is the activation function, which requires a matrix of weights, $W_l$, and a vector, $b_l$ for each layer. $g_l$ can be any of the functions detailed below.  \n",
    "- Theoretically, a deep neural network can capture any continuous function when activation functions are used.  \n",
    "\n",
    "Linear Activation (regression): \n",
    "$$\n",
    "\\phi(x) = z\n",
    "$$\n",
    "\n",
    "Sigmoid (logistic regression, ANN):\n",
    "$$\n",
    "\\phi(z)=\\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "Hyperbolic tangent - tanh (ANN):\n",
    "$$\n",
    "\\phi(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}\n",
    "$$\n",
    "\n",
    "ReLU (ANN):\n",
    "$$\n",
    "\\phi(z)= \\begin{cases}\n",
    "    0, & \\text{if}\\ z<0\\\\\n",
    "    z, & \\text{if}\\ z>0\n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "z = np.arange(-3.0, 3.0, 0.01)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def tanh(z):\n",
    "    return (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.where(z<0, 0, z)\n",
    "    \n",
    "plt.plot(z, sigmoid(z), 'b--')\n",
    "plt.plot(z, tanh(z), 'r--')\n",
    "plt.plot(z, relu(z), 'g--')\n",
    "plt.legend(['Sigmoid','tanh', 'ReLU'])\n",
    "plt.hlines(0, -3, 3)\n",
    "plt.vlines(0, -3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generally, `ReLu` is the default activation function that you should use, but it really could be treated `hyperparameter`. This is because its derivative is always 1 (derivative of a constant $=1$) for positive values so it addresses issues that arise from gradients that approach zero (`vanishing gradients`).  \n",
    "- The output layer will have a unit per class, which generalizes the `one-versus-all` technique (below image is for a 3-class problem). For a binary class, this output layer would have only one unit.  \n",
    "\n",
    "<img src='files/diagrams/12_03.png' style='width: 500px'>\n",
    "\n",
    "[Image source Machine Learning with Python 3rd Edition Pg. 390](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch12)\n",
    "\n",
    "\n",
    "- `Number of layers` and `number of units` are hyperparameters that will need to be tuned.  \n",
    "\n",
    "#### The `hidden layers` can represent \"concepts\" or \"features\" that aren't explicitly in the data, but the network might discover a latent approximation.\n",
    "You won't know what these are, but the neural net may infer them.\n",
    "\n",
    "<img src='files/diagrams/housing_price.png' style='width: 600px'>\n",
    "\n",
    "[Image source Andrew Ng](https://www.youtube.com/watch?v=n1l-9lIMW7E&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=3)\n",
    "\n",
    "### Basic Process\n",
    "- Start at input layer, propagate patterns of training data through to the output layer.  \n",
    "- Calculate the error we want to minimize (cost function). Similar to how we implemented the gradient descent algorithm in the past.    \n",
    "- **NEW** - Backpropagate error by finding its derivative to each weight and update the model. The below diagram illusrates backpropagation:  \n",
    "\n",
    "<img src='files/diagrams/12_12-back.png' style='width: 600px'>\n",
    "\n",
    "[Image source Machine Learning with Python 3rd Edition Pg. 421](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch12)\n",
    "\n",
    "- See [numpy's examples](https://numpy.org/numpy-tutorials/content/tutorial-deep-learning-on-mnist.html) for a manual implementation of a neural network.  \n",
    "- Typically use `mini-batch` for training - samples of the data during each weight update.  \n",
    "- Training can be more complicated because there can be millions of weights that need to be trained with deep neural networks. Getting stuck in `local minimas` can be a problem.\n",
    "\n",
    "<img src='files/diagrams/12_13-cost.png' style='width: 500px'>\n",
    "\n",
    "[Image source Machine Learning with Python 3rd Edition Pg. 422](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch12)\n",
    "\n",
    "\n",
    "### Implementations and Frameworks\n",
    "\n",
    "- [scikit-learn has an implementation](https://scikit-learn.org/stable/modules/neural_networks_supervised.html) but it isn't intended for large-scale problem since scikit-learn doesn't have GPU support.  \n",
    "- Most common packages/frameworks:  \n",
    "    - [TensorFlow](https://www.tensorflow.org)  \n",
    "    - [Keras](https://keras.io)  \n",
    "    - [PyTorch](https://pytorch.org)  \n",
    "    - [mxnet](https://mxnet.apache.org/versions/1.8.0/)  \n",
    "- We'll show a simple examle using scikit learn's implementation, then focus on `keras`, since the APIs available are relatively simple compared to the other frameworks.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning\n",
    "Tuning can be very complicated for ANNs, especially for deep neural networks. Potential hyperparameters:\n",
    "- Number of layers - additional layers instead of more units might be more effective.  \n",
    "- Number of units.  \n",
    "- Activation function `at each layer`.  \n",
    "- Weight initialization (typically random, but can be anything).  \n",
    "- Solver.  \n",
    "- Batch size - if you have a GPU, use the largest batch than fits on the GPU for maximum performance.  \n",
    "- Learning rate - arguably the most important.  \n",
    "- Iterations.  \n",
    "- Shape of deeper layers.  \n",
    "- Convolutions and filters.  \n",
    "- Wide and deep?  \n",
    "- ...  \n",
    "\n",
    "This is why it takes a lot of effort and compute to train certain models. Since there are so many potential levers for performance gains, usually `improvements` are released iteratively.\n",
    "\n",
    "[A Disciplined Approach to Neural Network Hyper-Parameters](https://arxiv.org/abs/1803.09820)  \n",
    "- No perfect way to go about looking for the best hyper-parameters.  \n",
    "- Grid or Random search is an option, but computationally expensive.  \n",
    "- Many cheat by using an existing model, e.g., AlexNet.  \n",
    "- Best practices:\n",
    "    - Look at the loss curves for clues about the suitability of the learning rate (under/over-fitting).  \n",
    "    - Batch size is somewhat constrained by your computing environment (data $\\le$ memory).  \n",
    "    - Smaller batches add a little regularization.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Implementation\n",
    "[Based on Alex Trask's `Grokking Deep Learning`](https://github.com/iamtrask/Grokking-Deep-Learning)\n",
    "\n",
    "**Learning with Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# inital weights\n",
    "weights = np.array([0.5,0.48,-0.7])\n",
    "\n",
    "# learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "# data on street lights\n",
    "streetlights = np.array( [[ 1, 0, 1 ],\n",
    "                          [ 0, 1, 1 ],\n",
    "                          [ 0, 0, 1 ],\n",
    "                          [ 1, 1, 1 ],\n",
    "                          [ 0, 1, 1 ],\n",
    "                          [ 1, 0, 1 ] ] )\n",
    "\n",
    "# target variable: cross the street or not\n",
    "walk_vs_stop = np.array( [ 0, 1, 0, 1, 1, 0 ] )\n",
    "\n",
    "# starting points\n",
    "input = streetlights[0]\n",
    "goal_prediction = walk_vs_stop[0]\n",
    "\n",
    "weights_ = list()\n",
    "preds_   = list()\n",
    "errors_  = list()\n",
    "\n",
    "# gradient descent to learn weights with 40 iterations\n",
    "for iteration in range(40):\n",
    "    \n",
    "    # initial error\n",
    "    error_for_all_lights = 0\n",
    "    \n",
    "    # loop through rows\n",
    "    for row_index in range(len(walk_vs_stop)):\n",
    "        # each observation/epoch will update weights\n",
    "        input = streetlights[row_index]\n",
    "        goal_prediction = walk_vs_stop[row_index]\n",
    "        \n",
    "        # dot product of input and weights\n",
    "        prediction = input.dot(weights)\n",
    "        \n",
    "        # error of prediction against observed\n",
    "        error = (goal_prediction - prediction) ** 2\n",
    "        error_for_all_lights += error\n",
    "        \n",
    "        # gradient and weight update\n",
    "        delta = prediction - goal_prediction\n",
    "        weights = weights - (alpha * (input * delta))\n",
    "\n",
    "        weights_.append(weights)\n",
    "        preds_.append(prediction)\n",
    "        \n",
    "    errors_.append(error_for_all_lights)\n",
    "    \n",
    "plt.plot(weights_)\n",
    "plt.title('Weight Updates - Updates for Each Observation/Epoch', loc='left')\n",
    "plt.legend(['Feature 1', 'Feature 2', 'Feature 3'])\n",
    "plt.xlabel('Update iteration (Observation * Epoch)')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(errors_)\n",
    "plt.title('Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Hidden Layers and Units\n",
    "- 3 Features x 4 Hidden Units = 12 weights from first layer  \n",
    "- 4 hidden units. \n",
    "\n",
    "> 16 weights for the below example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# activation function for each weight\n",
    "def relu(x):\n",
    "    return (x > 0) * x \n",
    "\n",
    "# output function\n",
    "def relu2deriv(output):\n",
    "    return output>0\n",
    "\n",
    "# learning rate and number of hidden units\n",
    "alpha = 0.2\n",
    "hidden_size = 4\n",
    "\n",
    "# training data\n",
    "streetlights = np.array( [[ 1, 0, 1 ],\n",
    "                          [ 0, 1, 1 ],\n",
    "                          [ 0, 0, 1 ],\n",
    "                          [ 1, 1, 1 ] ] )\n",
    "\n",
    "# labels\n",
    "walk_vs_stop = np.array([[ 1, 1, 0, 0]]).T\n",
    "\n",
    "# initial weights for layers\n",
    "weights_0_1 = 2*np.random.random((3, hidden_size)) - 1\n",
    "weights_1_2 = 2*np.random.random((hidden_size, 1)) - 1\n",
    "\n",
    "# input layer, initial hidden layer, output layer\n",
    "layer_0 = streetlights[0]\n",
    "layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "layer_2 = np.dot(layer_1, weights_1_2)\n",
    "\n",
    "# epochs and storeage\n",
    "epochs = 60\n",
    "errors_ = list()\n",
    "\n",
    "from collections import defaultdict\n",
    "l2_1_weights_ = defaultdict(list)\n",
    "l1_0_weights_ = defaultdict(list)\n",
    "\n",
    "for iteration in range(epochs):\n",
    "\n",
    "    # initial error\n",
    "    layer_2_error = 0\n",
    "    \n",
    "    for i in range(len(streetlights)):\n",
    "        # weights for each layer (input -> hidden -> output)\n",
    "        layer_0 = streetlights[i:i+1]\n",
    "        layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "        layer_2 = np.dot(layer_1,weights_1_2)\n",
    "\n",
    "        # errors\n",
    "        layer_2_error += np.sum((layer_2 - walk_vs_stop[i:i+1]) ** 2)\n",
    "        \n",
    "        # error gradient\n",
    "        layer_2_delta = (walk_vs_stop[i:i+1] - layer_2)\n",
    "        # error gradient in hidden layer\n",
    "        layer_1_delta=layer_2_delta.dot(weights_1_2.T)*relu2deriv(layer_1)\n",
    "        \n",
    "        # update weights (output -> hidden, then hidden -> input, i.e., backwards)\n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "        \n",
    "        errors_.append(layer_2_error)\n",
    "        \n",
    "        # store weight values\n",
    "        for i in range(np.size(weights_1_2)):\n",
    "            l2_1_weights_[i].append(weights_1_2[i][0])\n",
    "        \n",
    "        for i in range(np.size(weights_0_1)):   \n",
    "            l1_0_weights_[i].append(weights_0_1.reshape(12,)[i])\n",
    "            \n",
    "print('Completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "l0_1 = pd.DataFrame(l1_0_weights_)\n",
    "l0_1.plot(legend=None)\n",
    "plt.title('Input to Hidden Layer Weights', loc='left')\n",
    "plt.xlabel('Iteration of Weight Update (Example/Epoch)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_1 = pd.DataFrame(l2_1_weights_)\n",
    "l2_1.plot()\n",
    "plt.title('Hidden Unit Weights', loc='left')\n",
    "plt.xlabel('Iteration of Weight Update (Example/Epoch)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(errors_)\n",
    "plt.title('Loss', loc='left')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks with the MNIST Data\n",
    "We'll baseline with a `logistic regression` so we understand what type of performance gain the neural network is providing us.\n",
    "\n",
    "- 784 input neurons. \n",
    "- 10 output neurons (1 for each class).  \n",
    "- 7,840 parameters to learn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import datetime\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, parser='auto')\n",
    "\n",
    "en = datetime.datetime.now()\n",
    "el = en - st\n",
    "\n",
    "print(f'Fetch time: {el}')\n",
    "\n",
    "X = X / 255.0\n",
    "\n",
    "# rescale the data, use the traditional train/test split\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "print(f'Training examples: {X_train.shape[0]:,}')\n",
    "print(f'Test examples: {X_test.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Let's get a baseline from a vanilla logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import datetime\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr = lr.fit(X_train, y_train)\n",
    "\n",
    "en = datetime.datetime.now()\n",
    "el = en - st\n",
    "\n",
    "print(f'Fit time: {el}')\n",
    "print(f'Training set score: {lr.score(X_train, y_train):.2%}')\n",
    "print(f'Test score: {lr.score(X_test, y_test):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [scikit-Learn Multi-layer Perceptron](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n",
    "Using the MNIST dataset, we'll train a neural net with `1-hidden layer` that contains `50-hidden units`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import datetime\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(50,),\n",
    "    max_iter=100,\n",
    "    alpha=1e-4,\n",
    "    solver=\"sgd\",\n",
    "    verbose=0,\n",
    "    random_state=1,\n",
    "    learning_rate_init=0.1,\n",
    ")\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "en = datetime.datetime.now()\n",
    "\n",
    "print(f'Training set score: {mlp.score(X_train, y_train):.2%}')\n",
    "print(f'Test score: {mlp.score(X_test, y_test):.2%}')\n",
    "print(f'\\nTraining completed in {en-st}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mlp.loss_curve_)\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Training stopped because the loss converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(mlp, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Deep\" Network on MNIST\n",
    "- ANNs with more than 2 hidden layers are consider `deep` networks.  \n",
    "- The example below will have 3 hidden layers, but it isn't uncommon for more complicated models to have hundreds of hidden layers, including ones that are more complicated than simple direct input/outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import datetime\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(50,50,50),\n",
    "    max_iter=100,\n",
    "    alpha=1e-4,\n",
    "    solver=\"sgd\",\n",
    "    verbose=0,\n",
    "    random_state=1,\n",
    "    learning_rate_init=0.1\n",
    ")\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "en = datetime.datetime.now()\n",
    "\n",
    "print(f'Training completed in {en-st}')\n",
    "print(f'Training set score: {mlp.score(X_train, y_train):.2%}')\n",
    "print(f'Test score: {mlp.score(X_test, y_test):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mlp.loss_curve_)\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.n_layers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `Input` + `Hidden Layer 1` + `Hidden Layer 2` + `Hidden Layer 3` + `Output Layer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The loss function is showing `a lot more roughness` here. Recall it has been smooth when we've looked at other models. If these gyrations where larger, it is possible the model could have gotten stuck in a local minima. The risk of that happening with deep neural networks is substantially larger than for the simpler models we have looked at previously.\n",
    "\n",
    "- Training a model like this wasn't possible 35 years ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(mlp, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Single layer outperformed a multi-layer network. Deep isn't necessarily better - you need to grid search through the layers and units to determine the `best` values. In this case, a single 50-unit hidden layer performed about the same as two 50-unit hidden layers; so simple wins once again.\n",
    "\n",
    "### Our `Deep Learning` Model Didn't Improve MNIST Performance\n",
    "#### However, there's been significant progress on Deep Learning Models: \n",
    "\n",
    "<img src='files/diagrams/dnn-comparison.png'>\n",
    "\n",
    "#### And Architectures Can be Complicated\n",
    "\n",
    "<img src='files/diagrams/sample-dnn.png'>\n",
    "\n",
    "[AlexNet](https://en.wikipedia.org/wiki/AlexNet)\n",
    "<br>[VGG (Very Deep Convolutional Networks for Large-Scale Image Recognition)](https://arxiv.org/abs/1409.1556)\n",
    "<br>[GoogleNet](https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf)\n",
    "> You can download pre-trained versions of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Time\n",
    "- Each model took about 1 minute to run.  \n",
    "- Say we want to evaluate up to a 3-layer network.  \n",
    "- We have no prior on the number of units so, we'd need to try several.  \n",
    "- Grid could look like this:  \n",
    "    - {'hidden_layer_sizes': (100,), learning_rate = 0.0001}. \n",
    "    - {'hidden_layer_sizes': (75,), learning_rate = 0.0001}.  \n",
    "    - {'hidden_layer_sizes': (50,), learning_rate = 0.0001}.\n",
    "    - {'hidden_layer_sizes': (100,), learning_rate = 0.001}. \n",
    "    - {'hidden_layer_sizes': (75,), learning_rate = 0.001}.  \n",
    "    - {'hidden_layer_sizes': (50,), learning_rate = 0.001}.\n",
    "    - {'hidden_layer_sizes': (100, 100,), learning_rate = 0.0001}. \n",
    "    - {'hidden_layer_sizes': (75, 75, ), learning_rate = 0.0001}.  \n",
    "    - {'hidden_layer_sizes': (50, 50, ), learning_rate = 0.0001}.\n",
    "    - {'hidden_layer_sizes': (100, 100, ), learning_rate = 0.001}. \n",
    "    - {'hidden_layer_sizes': (75, 75, ), learning_rate = 0.001}.  \n",
    "    - {'hidden_layer_sizes': (50, 50, ), learning_rate = 0.001}.  \n",
    "    - {'hidden_layer_sizes': (100, 75,), learning_rate = 0.0001}. \n",
    "    - {'hidden_layer_sizes': (75, 50, ), learning_rate = 0.0001}.  \n",
    "    - {'hidden_layer_sizes': (50, 25, ), learning_rate = 0.0001}.\n",
    "    - {'hidden_layer_sizes': (100, 75, ), learning_rate = 0.001}. \n",
    "    - {'hidden_layer_sizes': (75, 50, ), learning_rate = 0.001}.  \n",
    "    - {'hidden_layer_sizes': (50, 25, ), learning_rate = 0.001}.  \n",
    "    - ...\n",
    "\n",
    "> Need to look to frameworks that allow for more efficient computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Keras](https://keras.io)\n",
    "One of the more popular frameworks, that can scale to GPUs or TPUs, tl;dr it can run models fast (relatively speaking). Developed by `Francois Chollet`, who literally wrote the book on [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python).\n",
    "\n",
    "Keras serves as a wrapper on [TensorFlow](https://www.tensorflow.org), but not all of TensorFlow's functionality is available in the Keras API.\n",
    "\n",
    "GridSearch and Hyperparameter tuning can be done another package, [KerasTuner](https://keras.io/keras_tuner/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Regression Example\n",
    "- Similar to many of the models we've talked about, you can also use ANNs for regression problems.  \n",
    "- Probably don't want to use activation functions for the outputs so the values are continuous and unbounded.  \n",
    "- Loss is generally `mean squared error`, but another variant may be better dependent on your data, e.g., lots of outliers may warrant `mean absolute error` as a better choice.  \n",
    "- Most regression problems will require 5 or fewer layers, with most requiring only 1 or 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X_train = np.arange(10).reshape((10,1))\n",
    "y_train = np.array([1.0, 1.3, 3.1, 2.0, 5.0, 6.3, 6.6, 7.4, 8.0, 9.0])\n",
    "\n",
    "X_train_norm = (X_train - np.mean(X_train))/np.std(X_train)\n",
    "\n",
    "plt.plot(X_train_norm, y_train, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify and fit the model\n",
    "Linear regression only one ouput layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "lm = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(1, activation='linear', input_dim=1, name='coef')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(lm.summary())\n",
    "\n",
    "lm.compile(loss=\"mse\", optimizer=\"sgd\", metrics=['mae', 'mse'])\n",
    "\n",
    "history = lm.fit(X_train_norm, y_train, epochs=100, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_loss, h_mse = history.history['loss'], history.history['mse']\n",
    "\n",
    "plt.plot(h_loss)\n",
    "plt.title('Loss', loc='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train_norm, y_train, 'ro')\n",
    "plt.plot(X_train_norm, lm.predict(X_train_norm), 'b-')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Actual', 'Predicted'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with MNIST\n",
    "#### Still using the MNIST data, but sourcing it from Keras, it is stored in a slightly different format, but its the same data as in the scikit-learn example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "# 28 x 28 = 784\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Layer (aka `Logistic Regression`)\n",
    "We will sequentially add layers to the model.  \n",
    "- Input layer specifies the incoming data, i.e., the pixels.  \n",
    "- The next layer specifies the layers, which includes the number of units and the activation function.  \n",
    "- The final layer is the output layer, which is a `softmax` activation function since this is a multiclass problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Flatten(name='Flatten'),\n",
    "        layers.Dense(num_classes, activation=\"softmax\", name=\"Softmax\")\n",
    "    ], name='Logistic'\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions for Neural Networks\n",
    "\n",
    "- Sigmoid or softmax are the output layers (binary/multiclass) which output class probabilities.  \n",
    "- Without them, you'd output logits.  \n",
    "- Cross-entropy (binary/categorical) is the loss, but need to specify it dependent on your output:\n",
    "\n",
    "<img src='diagrams/14_11_loss.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", \n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, validation_split=0.1, verbose=0)\n",
    "\n",
    "en = datetime.datetime.now()\n",
    "el = en - st\n",
    "\n",
    "print(f'Elapsed time: {el}')\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Loss', loc='left')\n",
    "plt.ylim((0, 1))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Accuracy', loc='left')\n",
    "plt.ylim((0.7, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]:.3f}')\n",
    "print(f'Test accuracy: {score[1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Flatten(name='Flatten'),\n",
    "        layers.Dense(50, activation=\"relu\", name=\"Hidden\"),\n",
    "        layers.Dense(50, activation=\"relu\", name=\"Second\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\", name='Output'),\n",
    "    ], name='2_Hidden_Layers'\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `Need to learn 42,310 parameters!`\n",
    "\n",
    "Potential issues:\n",
    "- Vanishing gradients - they'll get smaller as the number of layers increase. May cause convergence issues.  \n",
    "- In rarer cases, the gradients may increase and explode towards infinity.  \n",
    "- Using ReLU makes these rarer; sigmoid is much more suspectible.  \n",
    "    - It is possible for ReLU to only output `0` and many of the neurons will `die`. Leaky ReLU, $\\phi(z)=max(\\alpha z, z)$ guards against this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sometimes you will need to normalize the output of the layer prior to moving it into the next layer, which is called `batch normalization`. It zero-centers and normalizes the outputs.\n",
    "\n",
    "```python\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Flatten(name='Flatten'),\n",
    "        layers.Dense(50, activation=\"relu\", name=\"Hidden\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(50, activation=\"relu\", name=\"Second\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(num_classes, activation=\"softmax\", name='Output'),\n",
    "    ], name='2-Hidden Layers'\n",
    ")\n",
    "```\n",
    "\n",
    "Unfortunately, `BatchNormalization` has additional hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", \n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, validation_split=0.1, verbose=0)\n",
    "\n",
    "en = datetime.datetime.now()\n",
    "\n",
    "el = en - st\n",
    "print(f'Elapsed time: {el}')\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Loss', loc='left')\n",
    "plt.ylim((0,1))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Accuracy', loc='left')\n",
    "plt.ylim((0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]:.3f}')\n",
    "print(f'Test accuracy: {score[1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizing\n",
    "- Each layer has support for $l_2$ or $l_1$ regularization. See [the documentation](https://keras.io/api/layers/regularizers/).  \n",
    "- Can also use `dropout`, which means we will drop a neuron with probability $p$ during a training step.  \n",
    "    - Usually works better for earlier layers.  \n",
    "    - Like $l_1$ and $l_2$, try if the model is overfitting.\n",
    "  \n",
    "#### Before\n",
    "<img src='files/diagrams/holdout-before.png' style='height: 300px'>\n",
    "\n",
    "#### After\n",
    "<img src='files/diagrams/holdout-after.png' style='height: 300px'>\n",
    "\n",
    "\n",
    "Example code:\n",
    "```python\n",
    "    keras.layers.Dropout(rate=0.2)\n",
    "```\n",
    "\n",
    "- If `dropout` after each layer is causing underfitting, try using with last layer only.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Flatten(name='Flatten'),\n",
    "        layers.Dense(50, activation=\"relu\", name=\"Hidden\"),\n",
    "        layers.Dense(50, activation=\"relu\", name=\"Second\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\", name='Output'),\n",
    "    ], name='2_Hidden_Layers'\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", \n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, validation_split=0.1, verbose=0)\n",
    "\n",
    "en = datetime.datetime.now()\n",
    "\n",
    "el = en - st\n",
    "print(f'Elapsed time: {el}')\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Loss', loc='left')\n",
    "plt.ylim((0,1))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Accuracy', loc='left')\n",
    "plt.ylim((0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]:.3f}')\n",
    "print(f'Test accuracy: {score[1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonsequential Networks (Wide & Deep)\n",
    "Allows for learning simple and deep rules.\n",
    "\n",
    "- [Example on Keras's website](https://keras.io/examples/structured_data/wide_deep_cross_networks/)  \n",
    "- [Wide and Deep for Recommendation Systems](https://arxiv.org/abs/1606.07792)  \n",
    "- [Google Article on Wide & Deep](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html)\n",
    "\n",
    "<img src='diagrams/wide_deep.png' style='width: 900px'>\n",
    "\n",
    "[Image Source: Google AI Blog: Wide & Deep Learning: Better Together](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide & Deep on California Housing Data (Regression)\n",
    "Pages 307-310, Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "housing.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into training, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size = 0.20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.20)\n",
    "\n",
    "print(f'Training examples: {X_train.shape[0]:,}')\n",
    "print(f'Validation examples: {X_val.shape[0]:,}')\n",
    "print(f'Test examples: {X_test.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize - rules determined on training and applied to validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_tes = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wide & Deep Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "input_  = layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "hidden3 = layers.Dense(30, activation=\"relu\")(hidden2)\n",
    "concat  = layers.Concatenate()([input_, hidden3])\n",
    "output  = layers.Dense(1)(concat)\n",
    "\n",
    "model = keras.Model(inputs=[input_], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Input funnels to 1st, 2nd, and 3rd Hidden Layer; then is concatenated back with the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_val, y_val), verbose=0)\n",
    "\n",
    "en = datetime.datetime.now()\n",
    "el = en-st\n",
    "\n",
    "print(f'Elapsed time: {el}')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Loss', loc='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Different inputs can be used for the simple and shallow rules.  \n",
    "\n",
    "> You can also have multiple outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('california-housing.h5')\n",
    "\n",
    "import os\n",
    "print(*os.listdir(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('california-housing.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Really long training can use callbacks/check-points to save your models periodically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Parameters\n",
    "- The amount of hyperparameters and architecture options making tuning difficult and time consuming.  \n",
    "- Simple forward-feed model will have:\n",
    "    - number of layers. \n",
    "    - number of neurons per layer.  \n",
    "    - type of activation function per layer.  \n",
    "    - initialization logic.  \n",
    "    - learning rate.  \n",
    "    - ...\n",
    "- Can use scikit-learn's grid search to make some of this easier.  \n",
    "- You can use inspect part of a space, see if it looks promising and explore around it.  \n",
    "- Packages that can help:  \n",
    "    - Hyperopt  \n",
    "    - Hyperas  \n",
    "    - Keras Tuner  \n",
    "    - Scikit-Optimize  \n",
    "    - Spearmint  \n",
    "    - Sklearn-Deep\n",
    "    - Google Cloud has also options for tuning.  \n",
    "    \n",
    "[Paper on Uber's Evolutionary Approach](https://eng.uber.com/deep-neuroevolution/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    '''\n",
    "    Source page 320, Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow\n",
    "    '''\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "        \n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "params = {\n",
    "    'n_hidden': (1, 2),\n",
    "    'n_neurons': (1,2,3,4,5,6,7,9,10,11,12,13,14,15,16,17,18,19,20)\n",
    "}\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, params, n_iter=5, cv=3)\n",
    "rnd_search_cv = rnd_search_cv.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=0)\n",
    "\n",
    "en = datetime.datetime.now()\n",
    "el = en - st\n",
    "print(f'Elapsed time: {el}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Types of Advanced Neural Networks\n",
    "- [Convolutional Networks](https://www.ibm.com/cloud/learn/convolutional-neural-networks)  \n",
    "- [Autoencoders](https://en.wikipedia.org/wiki/Autoencoder)  \n",
    "- [Recurrent Neural Networks](https://en.wikipedia.org/wiki/Recurrent_neural_network)  \n",
    "- [Generative Adversarial Networks](https://en.wikipedia.org/wiki/Generative_adversarial_network) \n",
    "- [Graph Neural Networks](https://arxiv.org/pdf/1812.08434.pdf)  \n",
    "- [Autoencoders](https://en.wikipedia.org/wiki/Autoencoder)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
